{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font size=\"5\" color=\"red\"><b>News Summarization Project</b></font>\n\n\n<font size=\"4\">Welcome to the News Summarization project! In this project, we aim to automatically generate concise and informative summaries for news articles using Natural Language Processing (NLP) and fine-tuning pre-trained Large Language Model (LLM).</font>","metadata":{}},{"cell_type":"markdown","source":"Project Overview:\n\n- Objective: Create an automatic news summarization tool using the T5 model.\n- Steps: Data Preparation, Preprocessing, Model Fine-Tuning, and Model Deployment.\n- Benefits: Save time by getting key points from news articles without reading the entire   content.\n\nUsage:\n\n1. Load news articles dataset.\n2. Preprocess dataset and create a custom dataset.\n3. Fine-tune T5 model for news summarization.\n4. Deploy the model using Gradio for interactive summarization.\n\nFeel free to customize hyperparameters and experiment for the best results.\n\nLet's start building an automated news summarization tool!","metadata":{}},{"cell_type":"code","source":"#Install required libraries\n! pip install -q transformers accelerate sentencepiece gradio","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:24:04.406371Z","iopub.execute_input":"2023-08-13T22:24:04.406718Z","iopub.status.idle":"2023-08-13T22:24:24.369046Z","shell.execute_reply.started":"2023-08-13T22:24:04.406688Z","shell.execute_reply":"2023-08-13T22:24:24.367625Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Importing stock libraries\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n\n# Importing the T5 modules from huggingface/transformers\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:24:24.373235Z","iopub.execute_input":"2023-08-13T22:24:24.373540Z","iopub.status.idle":"2023-08-13T22:24:36.502176Z","shell.execute_reply.started":"2023-08-13T22:24:24.373513Z","shell.execute_reply":"2023-08-13T22:24:36.501188Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import the 'cuda' module from the 'torch' library to check if GPU (CUDA) support is available\nfrom torch import cuda\n# The 'device' variable now indicates whether the code will run on GPU ('cuda') or CPU ('cpu')\n# You can use this 'device' variable to move tensors and models to the appropriate device for computation\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:24:36.503364Z","iopub.execute_input":"2023-08-13T22:24:36.503688Z","iopub.status.idle":"2023-08-13T22:24:36.532842Z","shell.execute_reply.started":"2023-08-13T22:24:36.503657Z","shell.execute_reply":"2023-08-13T22:24:36.530842Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Read the CSV file 'news_summary.csv' from the specified path and use 'latin-1' encoding\ndf = pd.read_csv('/kaggle/input/news-summary/news_summary.csv',encoding='latin-1')\n# Select only the 'text' and 'ctext' columns from the DataFrame\ndf = df[['text','ctext']]\n# Prepend 'summarize: ' to the 'ctext' column in the DataFrame\ndf.ctext = 'summarize: ' + df.ctext","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:24:36.535913Z","iopub.execute_input":"2023-08-13T22:24:36.536622Z","iopub.status.idle":"2023-08-13T22:24:36.870620Z","shell.execute_reply.started":"2023-08-13T22:24:36.536581Z","shell.execute_reply":"2023-08-13T22:24:36.869649Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Create a training dataset by randomly sampling 1000 rows from the DataFrame 'df'\n# The 'random_state' parameter ensures reproducibility of the random sampling\n# Reset the index of the sampled dataset and drop the previous index column\ntrain_dataset=df.sample(1000, random_state = 42).reset_index().drop('index', axis=1)\n# Drop any rows containing missing values (NaN) from the training dataset\ntrain_dataset = train_dataset.dropna()","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:24:36.871924Z","iopub.execute_input":"2023-08-13T22:24:36.874351Z","iopub.status.idle":"2023-08-13T22:24:36.886536Z","shell.execute_reply.started":"2023-08-13T22:24:36.874322Z","shell.execute_reply":"2023-08-13T22:24:36.885582Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(\"FULL Dataset: {}\".format(df.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:24:36.888345Z","iopub.execute_input":"2023-08-13T22:24:36.888793Z","iopub.status.idle":"2023-08-13T22:24:36.895141Z","shell.execute_reply.started":"2023-08-13T22:24:36.888742Z","shell.execute_reply":"2023-08-13T22:24:36.893675Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"FULL Dataset: (4514, 2)\nTRAIN Dataset: (976, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a tokenizer instance using the pre-trained 't5-base' tokenizer\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:24:36.896788Z","iopub.execute_input":"2023-08-13T22:24:36.897183Z","iopub.status.idle":"2023-08-13T22:24:39.775854Z","shell.execute_reply.started":"2023-08-13T22:24:36.897147Z","shell.execute_reply":"2023-08-13T22:24:39.774602Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e199c62ca6ad4f3e9138d96dfb802df2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa4099d52e5f421d98134c192f7dd09e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating a custom dataset class for loading dataframe into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n\nclass CustomDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.source_len = source_len\n        self.summ_len = summ_len\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, index):\n        ctext = str(self.data.iloc[index]['ctext'])\n        \n        text = str(self.data.iloc[index]['text'])\n\n        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt', truncation = True)\n        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt', truncation = True)\n\n        source_ids = source['input_ids'].squeeze()\n        source_mask = source['attention_mask'].squeeze()\n        target_ids = target['input_ids'].squeeze()\n        target_mask = target['attention_mask'].squeeze()\n\n        return {\n            'source_ids': source_ids.to(dtype=torch.long), \n            'source_mask': source_mask.to(dtype=torch.long), \n            'target_ids': target_ids.to(dtype=torch.long),\n            'target_ids_y': target_ids.to(dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:24:39.777661Z","iopub.execute_input":"2023-08-13T22:24:39.778363Z","iopub.status.idle":"2023-08-13T22:24:39.790086Z","shell.execute_reply.started":"2023-08-13T22:24:39.778321Z","shell.execute_reply":"2023-08-13T22:24:39.788674Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Create a training dataset using the CustomDataset class\ntraining_set = CustomDataset(train_dataset, tokenizer, 500, 125)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:24:39.793155Z","iopub.execute_input":"2023-08-13T22:24:39.793809Z","iopub.status.idle":"2023-08-13T22:24:39.815669Z","shell.execute_reply.started":"2023-08-13T22:24:39.793771Z","shell.execute_reply":"2023-08-13T22:24:39.814361Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Create a training data loader using the DataLoader class\n# Each batch will contain 10 samples, and the order of samples is shuffled\n# The 'num_workers' parameter controls the number of parallel data loading processes (set to 0 for single-process loading)\ntraining_loader = DataLoader(training_set, batch_size = 10, shuffle =  True, num_workers = 0)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:24:39.820412Z","iopub.execute_input":"2023-08-13T22:24:39.820706Z","iopub.status.idle":"2023-08-13T22:24:39.826466Z","shell.execute_reply.started":"2023-08-13T22:24:39.820680Z","shell.execute_reply":"2023-08-13T22:24:39.825331Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Create a T5 model instance using the pre-trained 't5-base' model\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n# Move the model to the specified device (GPU if available, else CPU)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:24:39.827984Z","iopub.execute_input":"2023-08-13T22:24:39.828368Z","iopub.status.idle":"2023-08-13T22:24:57.942793Z","shell.execute_reply.started":"2023-08-13T22:24:39.828319Z","shell.execute_reply":"2023-08-13T22:24:57.941783Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44ac2004c7544ab99e2ce7ce9f354a78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3a1d2ecd044498884ce0b93584b5d94"}},"metadata":{}}]},{"cell_type":"code","source":"# Create an optimizer instance using the Adam optimizer\noptimizer = torch.optim.Adam(params =  model.parameters(), lr= 3e-5)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:24:57.944289Z","iopub.execute_input":"2023-08-13T22:24:57.944640Z","iopub.status.idle":"2023-08-13T22:24:57.952424Z","shell.execute_reply.started":"2023-08-13T22:24:57.944603Z","shell.execute_reply":"2023-08-13T22:24:57.951398Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Creating the training function. This will be called later. It is run depending on the epoch value.\n# The model is put into train mode and then we enumerate over the training loader and passed to the defined network \n\ndef train(epoch, tokenizer, model, device, loader, optimizer):\n    model.train()\n    for _,data in enumerate(loader, 0):\n        y = data['target_ids'].to(device, dtype = torch.long)\n        y_ids = y[:, :-1].contiguous()\n        lm_labels = y[:, 1:].clone().detach()\n        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n        ids = data['source_ids'].to(device, dtype = torch.long)\n        mask = data['source_mask'].to(device, dtype = torch.long)\n\n        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n        loss = outputs[0]\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'training of epoch {epoch} ended with loss = {loss.item()}')","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:24:57.953928Z","iopub.execute_input":"2023-08-13T22:24:57.954260Z","iopub.status.idle":"2023-08-13T22:25:00.147229Z","shell.execute_reply.started":"2023-08-13T22:24:57.954228Z","shell.execute_reply":"2023-08-13T22:25:00.146104Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Iterate through a specified number of epochs for training\nfor epoch in range(3):\n    train(epoch, tokenizer, model, device, training_loader, optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:25:00.148580Z","iopub.execute_input":"2023-08-13T22:25:00.148971Z","iopub.status.idle":"2023-08-13T22:29:12.915386Z","shell.execute_reply.started":"2023-08-13T22:25:00.148937Z","shell.execute_reply":"2023-08-13T22:29:12.914298Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"training of epoch 0 ended with loss = 1.5711437463760376\ntraining of epoch 1 ended with loss = 1.767512321472168\ntraining of epoch 2 ended with loss = 1.7535284757614136\n","output_type":"stream"}]},{"cell_type":"code","source":"# Specify the path to save the model's weights and configuration\nPATH = '/kaggle/working/model_weights'\n# Use the 'torch.save' function to save the model's state dictionary\n# The state dictionary contains the trained parameters of the model\ntorch.save({\n    'model_state_dict': model.state_dict()\n}, PATH)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:29:12.917675Z","iopub.execute_input":"2023-08-13T22:29:12.919057Z","iopub.status.idle":"2023-08-13T22:29:14.364722Z","shell.execute_reply.started":"2023-08-13T22:29:12.919020Z","shell.execute_reply":"2023-08-13T22:29:14.363756Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Load the saved checkpoint from the specified path\nckp = torch.load(PATH)\n# Access the keys of the loaded checkpoint\n# These keys correspond to the elements saved in the checkpoint\n# For example, 'model_state_dict' holds the trained model's parameters\nckp.keys()","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:29:14.366159Z","iopub.execute_input":"2023-08-13T22:29:14.366588Z","iopub.status.idle":"2023-08-13T22:29:14.933533Z","shell.execute_reply.started":"2023-08-13T22:29:14.366555Z","shell.execute_reply":"2023-08-13T22:29:14.932359Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"dict_keys(['model_state_dict'])"},"metadata":{}}]},{"cell_type":"code","source":"# Load a new instance of T5 model from the base pre-trained version\n# The model will initially have the architecture of the base T5 model\nsaved_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(\"cuda\")\n# Load the trained model's state dictionary from the loaded checkpoint\n# This will update the model's parameters with the trained values\nsaved_model.load_state_dict(ckp['model_state_dict'])\n# Set the model's mode to evaluation\n# This disables dropout and other training-specific behaviors\nsaved_model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:29:14.935279Z","iopub.execute_input":"2023-08-13T22:29:14.935686Z","iopub.status.idle":"2023-08-13T22:29:18.802482Z","shell.execute_reply.started":"2023-08-13T22:29:14.935652Z","shell.execute_reply":"2023-08-13T22:29:18.801572Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def generate(input_text):\n    # Tokenize the input text and convert it to input IDs\n  input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=500, truncation=True).input_ids.to(\"cuda\")\n    # Generate a summarized output using the saved model\n  output = saved_model.generate(input_ids, max_length=125, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n    # Decode the generated output into human-readable text\n  return tokenizer.decode(output[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:29:18.803967Z","iopub.execute_input":"2023-08-13T22:29:18.804302Z","iopub.status.idle":"2023-08-13T22:29:18.810212Z","shell.execute_reply.started":"2023-08-13T22:29:18.804269Z","shell.execute_reply":"2023-08-13T22:29:18.809141Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Define the input text for testing\nText = '''Lashkar-e-Taiba's Kashmir commander Abu Dujana was killed in an encounter in a village in Pulwama district of Jammu and Kashmir earlier this week. Dujana, who had managed to give the security forces a slip several times in the past, carried a bounty of Rs 15 lakh on his head.Reports say that Dujana had come to meet his wife when he was trapped inside a house in Hakripora village. Security officials involved in the encounter tried their best to convince Dujana to surrender but he refused, reports say.According to reports, Dujana rejected call for surrender from an Army officer. The Army had commissioned a local to start a telephonic conversation with Dujana. After initiating the talk, the local villager handed over the phone to the army officer.\"Kya haal hai? Maine kaha, kya haal hai (How are you. I asked, how are you)?\" Dujana is heard asking the officer. The officer replies: \"Humara haal chhor Dujana. Surrender kyun nahi kar deta. Tu galat kar rha hai (Why don't you surrender? You have married this girl. What you are doing isn't right.)\"When told that he is being used by Pakistani agencies as a pawn, Dujana, who sounded calm and unperturbed of the situation, said \"Hum nikley they shaheed hone. Main kya karu. Jisko game khelna hai, khelo. Kabhi hum aage, kabhi aap, aaj aapne pakad liya, mubarak ho aapko. Jisko jo karna hai karlo (I had left home for martyrdom. What can I do? Today you caught me. Congratulations. \"Surrender nahi kar sakta. Jo meri kismat may likha hoga, Allah wahi karega, theek hai? (I won't surrender. Allaah would do whatever is there in my fate)\" Dujana went on to say. Dujana, who belonged to Pakistan, was Lashkar-e-Taiba's divisional commander in south Kashmir. He was among the top 10 terrorists identified by the Indian Army in Jammu and Kashmir.With a Rs 15 lakh bounty on his head, Dujana was labelled an 'A++' terrorist - the top grade which was also given to Burhan Wani.Security forces received inputs that during the last few days he was frequenting the houses of his wife Rukaiya and girlfriend Shazia. Police was keeping a watch on both the houses. when it was confirmed he was present in his wife's house, security forces moved in to trap him.ALSO READ:After Abu Dujana, security forces prepare new hitlist of most wanted terroristsAbu Dujana encounter: Jilted lover turned police informer led security forces to LeT commander'''","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:29:18.811671Z","iopub.execute_input":"2023-08-13T22:29:18.812250Z","iopub.status.idle":"2023-08-13T22:29:18.826304Z","shell.execute_reply.started":"2023-08-13T22:29:18.812217Z","shell.execute_reply":"2023-08-13T22:29:18.825398Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Generate the summarized version using the generate function\ngenerate(Text)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:29:18.827810Z","iopub.execute_input":"2023-08-13T22:29:18.828482Z","iopub.status.idle":"2023-08-13T22:29:20.156699Z","shell.execute_reply.started":"2023-08-13T22:29:18.828448Z","shell.execute_reply":"2023-08-13T22:29:20.155677Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'Dujana, who had managed to give the security forces a slip several times in the past, was killed in an encounter in a village in Pulwama district of Jammu and Kashmir earlier this week. \"Why don\\'t you surrender? You have married this girl. What you are doing isn\\'t right.\"'"},"metadata":{}}]},{"cell_type":"code","source":"# Import the Gradio library for creating interactive interfaces\nimport gradio as gr\n# Create a Gradio interface\niface = gr.Interface(\n    fn=generate,\n    inputs = gr.inputs.Textbox(lines=10, label=\"Input News Article\"),\n    outputs = gr.outputs.Textbox(label=\"Summarized News\"),\n    title=\"News Summarization: This website is made by Vivek Kumar\",\n    description=\"Enter a news article to get a summarized version.\"\n)\n\n# Deploy and share the interface\niface.launch(share=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T22:29:20.158222Z","iopub.execute_input":"2023-08-13T22:29:20.158602Z","iopub.status.idle":"2023-08-13T22:29:27.028902Z","shell.execute_reply.started":"2023-08-13T22:29:20.158568Z","shell.execute_reply":"2023-08-13T22:29:27.027822Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/320937009.py:6: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n  inputs = gr.inputs.Textbox(lines=10, label=\"Input News Article\"),\n/tmp/ipykernel_28/320937009.py:6: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n  inputs = gr.inputs.Textbox(lines=10, label=\"Input News Article\"),\n/tmp/ipykernel_28/320937009.py:6: GradioDeprecationWarning: `numeric` parameter is deprecated, and it has no effect\n  inputs = gr.inputs.Textbox(lines=10, label=\"Input News Article\"),\n/tmp/ipykernel_28/320937009.py:7: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n  outputs = gr.outputs.Textbox(label=\"Summarized News\"),\n","output_type":"stream"},{"name":"stdout","text":"Running on local URL:  http://127.0.0.1:7860\nRunning on public URL: https://2ea38289e3ac961dcf.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://2ea38289e3ac961dcf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}]}]}